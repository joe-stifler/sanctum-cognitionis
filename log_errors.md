Erro: Details: Traceback (most recent call last): File "/mnt/c/Users/josep/Documents/DocumentsWSL/1-PROJECTS/sanctum-cognitionis/app.py", line 211, in chat_messages new_chat_message = chat_history.send_ai_message( File "/mnt/c/Users/josep/Documents/DocumentsWSL/1-PROJECTS/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 50, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( File "/mnt/c/Users/josep/Documents/DocumentsWSL/1-PROJECTS/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/generativeai/generative_models.py", line 474, in send_message response = self.model.generate_content( File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/generativeai/generative_models.py", line 260, in generate_content return generation_types.GenerateContentResponse.from_iterator(iterator) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/generativeai/types/generation_types.py", line 451, in from_iterator response = next(iterator) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 116, in next return next(self._wrapped) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/grpc/_channel.py", line 543, in next return self._next() File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/grpc/_channel.py", line 950, in _next raise StopIteration() StopIteration

----


Erro: Please let the response complete iteration before accessing the final accumulated attributes (or call response.resolve()) Details: Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/app.py", line 209, in chat_messages new_chat_message = chat_history.send_ai_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 50, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 467, in send_message history = self.history[:] ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 677, in history if last.candidates[0].finish_reason not in ( ^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 322, in candidates raise IncompleteIterationError(_INCOMPLETE_ITERATION_MESSAGE) google.generativeai.types.generation_types.IncompleteIterationError: Please let the response complete iteration before accessing the final accumulated attributes (or call response.resolve())

----

Erro: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it. Details: Traceback (most recent call last): File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 116, in next return next(self._wrapped) ^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/grpc/_channel.py", line 543, in next return self._next() ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/grpc/_channel.py", line 969, in _next raise self grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with: status = StatusCode.DEADLINE_EXCEEDED details = "Deadline Exceeded" debug_error_string = "UNKNOWN
received from peer ipv4:74.125.20.95:443 {grpc_message:"Deadline Exceeded", grpc_status:4, created_time:"2024-05-06T17:00:31.092312913+00:00"}"

The above exception was the direct cause of the following exception:

Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 75, in process_ai_response_stream for chunk in responses: File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 480, in iter raise self._error File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 489, in iter item = next(self._iterator) ^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 119, in next raise exceptions.from_grpc_error(exc) from exc google.api_core.exceptions.DeadlineExceeded: 504 Deadline Exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/app.py", line 209, in chat_messages new_chat_message = chat_history.send_ai_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 50, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 467, in send_message history = self.history[:] ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 686, in history raise generation_types.BrokenResponseError( google.generativeai.types.generation_types.BrokenResponseError: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it.

----

Erro: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it. Details: Traceback (most recent call last): File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 116, in next return next(self._wrapped) ^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/grpc/_channel.py", line 543, in next return self._next() ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/grpc/_channel.py", line 969, in _next raise self grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with: status = StatusCode.DEADLINE_EXCEEDED details = "Deadline Exceeded" debug_error_string = "UNKNOWN
received from peer ipv4:74.125.197.95:443 {created_time:"2024-05-06T19:14:02.975304014+00:00", grpc_status:4, grpc_message:"Deadline Exceeded"}"

The above exception was the direct cause of the following exception:

Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 75, in process_ai_response_stream for chunk in responses: File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 480, in iter raise self._error File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 489, in iter item = next(self._iterator) ^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 119, in next raise exceptions.from_grpc_error(exc) from exc google.api_core.exceptions.DeadlineExceeded: 504 Deadline Exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/app.py", line 209, in chat_messages new_chat_message = chat_history.send_ai_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 50, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 467, in send_message history = self.history[:] ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 686, in history raise generation_types.BrokenResponseError( google.generativeai.types.generation_types.BrokenResponseError: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it.
