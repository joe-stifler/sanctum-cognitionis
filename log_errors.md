Erro: Details: Traceback (most recent call last): File "/mnt/c/Users/josep/Documents/DocumentsWSL/1-PROJECTS/sanctum-cognitionis/app.py", line 211, in chat_messages new_chat_message = chat_history.send_ai_message( File "/mnt/c/Users/josep/Documents/DocumentsWSL/1-PROJECTS/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 50, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( File "/mnt/c/Users/josep/Documents/DocumentsWSL/1-PROJECTS/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/generativeai/generative_models.py", line 474, in send_message response = self.model.generate_content( File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/generativeai/generative_models.py", line 260, in generate_content return generation_types.GenerateContentResponse.from_iterator(iterator) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/generativeai/types/generation_types.py", line 451, in from_iterator response = next(iterator) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 116, in next return next(self._wrapped) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/grpc/_channel.py", line 543, in next return self._next() File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/grpc/_channel.py", line 950, in _next raise StopIteration() StopIteration

----


Erro: Please let the response complete iteration before accessing the final accumulated attributes (or call response.resolve()) Details: Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/app.py", line 209, in chat_messages new_chat_message = chat_history.send_ai_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 50, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 467, in send_message history = self.history[:] ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 677, in history if last.candidates[0].finish_reason not in ( ^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 322, in candidates raise IncompleteIterationError(_INCOMPLETE_ITERATION_MESSAGE) google.generativeai.types.generation_types.IncompleteIterationError: Please let the response complete iteration before accessing the final accumulated attributes (or call response.resolve())

----

Erro: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it. Details: Traceback (most recent call last): File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 116, in next return next(self._wrapped) ^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/grpc/_channel.py", line 543, in next return self._next() ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/grpc/_channel.py", line 969, in _next raise self grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with: status = StatusCode.DEADLINE_EXCEEDED details = "Deadline Exceeded" debug_error_string = "UNKNOWN
received from peer ipv4:74.125.20.95:443 {grpc_message:"Deadline Exceeded", grpc_status:4, created_time:"2024-05-06T17:00:31.092312913+00:00"}"

The above exception was the direct cause of the following exception:

Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 75, in process_ai_response_stream for chunk in responses: File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 480, in iter raise self._error File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 489, in iter item = next(self._iterator) ^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 119, in next raise exceptions.from_grpc_error(exc) from exc google.api_core.exceptions.DeadlineExceeded: 504 Deadline Exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/app.py", line 209, in chat_messages new_chat_message = chat_history.send_ai_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 50, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 467, in send_message history = self.history[:] ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 686, in history raise generation_types.BrokenResponseError( google.generativeai.types.generation_types.BrokenResponseError: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it.

----

Erro: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it. Details: Traceback (most recent call last): File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 116, in next return next(self._wrapped) ^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/grpc/_channel.py", line 543, in next return self._next() ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/grpc/_channel.py", line 969, in _next raise self grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with: status = StatusCode.DEADLINE_EXCEEDED details = "Deadline Exceeded" debug_error_string = "UNKNOWN
received from peer ipv4:74.125.197.95:443 {created_time:"2024-05-06T19:14:02.975304014+00:00", grpc_status:4, grpc_message:"Deadline Exceeded"}"

The above exception was the direct cause of the following exception:

Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 75, in process_ai_response_stream for chunk in responses: File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 480, in iter raise self._error File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/types/generation_types.py", line 489, in iter item = next(self._iterator) ^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 119, in next raise exceptions.from_grpc_error(exc) from exc google.api_core.exceptions.DeadlineExceeded: 504 Deadline Exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/app.py", line 209, in chat_messages new_chat_message = chat_history.send_ai_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 50, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 467, in send_message history = self.history[:] ^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 686, in history raise generation_types.BrokenResponseError( google.generativeai.types.generation_types.BrokenResponseError: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it.

---

Segundo Marx, as transformações em uma sociedade de classes ocorrem através de lutas de classes, que podem ser pacíficas ou violentas. Essas lutas são motivadas pelas contradiçõesErro ao processar a mensagem: The response.text quick accessor only works when the response contains a valid Part, but none was returned. Check the candidate.safety_ratings to see if the response was blocked.

---

Erro: 504 Deadline Exceeded Details: Traceback (most recent call last): File "/mount/src/sanctum-cognitionis/app.py", line 235, in chat_messages new_chat_message = chat_history.send_ai_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 53, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/mount/src/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 474, in send_message response = self.model.generate_content( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/generativeai/generative_models.py", line 256, in generate_content iterator = self._client.stream_generate_content( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 1090, in stream_generate_content response = rpc( ^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py", line 131, in call return wrapped_func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py", line 293, in retry_wrapped_func return retry_target( ^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py", line 153, in retry_target _retry_error_helper( File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py", line 212, in _retry_error_helper raise final_exc from source_exc File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py", line 144, in retry_target result = target() ^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/timeout.py", line 120, in func_with_timeout return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File "/home/adminuser/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 174, in error_remapped_callable raise exceptions.from_grpc_error(exc) from exc google.api_core.exceptions.DeadlineExceeded: 504 Deadline Exceeded

---

Erro: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting Details: Traceback (most recent call last): File "/mnt/c/Users/josep/Documents/DocumentsWSL/1-PROJECTS/sanctum-cognitionis/app.py", line 233, in chat_messages new_chat_message = chat_history.send_ai_message( File "/mnt/c/Users/josep/Documents/DocumentsWSL/1-PROJECTS/sanctum-cognitionis/praesentatio_cognitionis/chat_history.py", line 53, in send_ai_message ai_response_stream = self.llm_model.send_stream_chat_message( File "/mnt/c/Users/josep/Documents/DocumentsWSL/1-PROJECTS/sanctum-cognitionis/servitium_cognitionis/llms/gemini/dev_models.py", line 61, in send_stream_chat_message ai_response_stream = self._model_chats[session_id].send_message(messages, stream=True) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/generativeai/generative_models.py", line 474, in send_message response = self.model.generate_content( File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/generativeai/generative_models.py", line 256, in generate_content iterator = self._client.stream_generate_content( File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 1090, in stream_generate_content response = rpc( File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in call return wrapped_func(*args, **kwargs) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 293, in retry_wrapped_func return retry_target( File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 153, in retry_target _retry_error_helper( File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 212, in _retry_error_helper raise final_exc from source_exc File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 144, in retry_target result = target() File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/api_core/timeout.py", line 120, in func_with_timeout return func(*args, **kwargs) File "/home/joe/anaconda3/envs/sanctum-cognitionis/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 174, in error_remapped_callable raise exceptions.from_grpc_error(exc) from exc google.api_core.exceptions.InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting
